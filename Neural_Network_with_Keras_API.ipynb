{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network with Keras API.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishekjha111/Hello-world/blob/master/Neural_Network_with_Keras_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cDIwEks2Pm6",
        "colab_type": "text"
      },
      "source": [
        "# Keras API\n",
        "\n",
        "Keras is a Python API for deep learning. It has something for everyone:\n",
        "\n",
        "If you're an engineer, Keras provides you with reusable blocks such as layers, metrics, training loops, to support common use cases. It provides a high-level user experience that's accessible and productive.\n",
        "\n",
        "If you're a researcher, you may prefer not to use these built-in blocks such as layers and training loops, and instead create your own. Of course, Keras allows you to do this. In this case, Keras provides you with templates for the blocks you write, it provides you with structure, with an API standard for things like Layers and Metrics. This structure makes your code easy to share with others and easy to integrate in production workflows.\n",
        "\n",
        "The same is true for library developers: TensorFlow is a large ecosystem. It has many different libraries. In order for different libraries to be able to talk to each other and share components, they need to follow an API standard. That's what Keras provides.\n",
        "\n",
        "Crucially, Keras brings high-level UX and low-level flexibility together fluently: you no longer have on one hand, a high-level API that's easy to use but inflexible, and on the other hand a low-level API that's flexible but only approachable by experts. Instead, you have a spectrum of workflows, from the very high-level to the very low-level. Workflows that are all compatible because they're built on top of the same concepts and objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfF45hSwF3Bl",
        "colab_type": "text"
      },
      "source": [
        "**The base Layer class**\n",
        "The first class you need to know is Layer. Pretty much everything in Keras derives from it.\n",
        "\n",
        "A Layer encapsulates a state (weights) and some computation (defined in the call method)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5nIEKdYGEhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MdI3dYD2DgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class Linear(Layer):\n",
        "  \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "  def __init__(self, units=32, input_dim=32):\n",
        "      super(Linear, self).__init__()\n",
        "      w_init = tf.random_normal_initializer()\n",
        "      self.w = tf.Variable(\n",
        "          initial_value=w_init(shape=(input_dim, units), dtype='float32'),\n",
        "          trainable=True)\n",
        "      b_init = tf.zeros_initializer()\n",
        "      self.b = tf.Variable(\n",
        "          initial_value=b_init(shape=(units,), dtype='float32'),\n",
        "          trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "# Instantiate our layer.\n",
        "linear_layer = Linear(4, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRVHgNICF_9K",
        "colab_type": "text"
      },
      "source": [
        "A layer instance works like a function. Let's call it on some data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqQOEa4eGAqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = linear_layer(tf.ones((2, 2)))\n",
        "assert y.shape == (2, 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kCpSGY_GLcr",
        "colab_type": "text"
      },
      "source": [
        "The Layer class takes care of tracking the weights assigned to it as attributes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJxiIIWuGMVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weights are automatically tracked under the `weights` property.\n",
        "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXPZu3qUGPOB",
        "colab_type": "text"
      },
      "source": [
        "Note that's also a shortcut method for creating weights: add_weight. Instead of doing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-upk5D9GSlO",
        "colab_type": "code",
        "outputId": "0eb0646b-cd6c-4807-b7b0-1302c3cd4e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "# dummy code\n",
        "w_init = tf.random_normal_initializer()\n",
        "self.w = tf.Variable(initial_value=w_init(shape=shape, dtype='float32'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-50a665efa918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'shape' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svw9SKBOGaWK",
        "colab_type": "text"
      },
      "source": [
        "You would typically do:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrvDxOrNGbEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dummy code\n",
        "self.w = self.add_weight(shape=shape, initializer='random_normal')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnIK1k-uGmLe",
        "colab_type": "text"
      },
      "source": [
        "Itâ€™s good practice to create weights in a separate build method, called lazily with the shape of the first inputs seen by your layer. Here, this pattern prevents us from having to specify input_dim in the constructor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4_pfaZ0GqKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(Layer):\n",
        "  \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "  def __init__(self, units=32):\n",
        "      super(Linear, self).__init__()\n",
        "      self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "      self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                               initializer='random_normal',\n",
        "                               trainable=True)\n",
        "      self.b = self.add_weight(shape=(self.units,),\n",
        "                               initializer='random_normal',\n",
        "                               trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "\n",
        "# Instantiate our lazy layer.\n",
        "linear_layer = Linear(4)\n",
        "\n",
        "# This will also call `build(input_shape)` and create the weights.\n",
        "y = linear_layer(tf.ones((2, 2)))\n",
        "assert len(linear_layer.weights) == 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ5qNtldG4aN",
        "colab_type": "text"
      },
      "source": [
        "## Trainable and non-trainable weights\n",
        "Weights created by layers can be either trainable or non-trainable. They're exposed in trainable_weights and non_trainable_weights. Here's a layer with a non-trainable weight:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcGpmkCxG6VS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ComputeSum(Layer):\n",
        "  \"\"\"Returns the sum of the inputs.\"\"\"\n",
        "\n",
        "  def __init__(self, input_dim):\n",
        "      super(ComputeSum, self).__init__()\n",
        "      # Create a non-trainable weight.\n",
        "      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n",
        "                               trainable=False)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
        "      return self.total  \n",
        "\n",
        "my_sum = ComputeSum(2)\n",
        "x = tf.ones((2, 2))\n",
        "\n",
        "y = my_sum(x)\n",
        "print(y.numpy())  # [2. 2.]\n",
        "\n",
        "y = my_sum(x)\n",
        "print(y.numpy())  # [4. 4.]\n",
        "\n",
        "assert my_sum.weights == [my_sum.total]\n",
        "assert my_sum.non_trainable_weights == [my_sum.total]\n",
        "assert my_sum.trainable_weights == []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS_gEQ-mHChJ",
        "colab_type": "text"
      },
      "source": [
        "## Recursively composing layers\n",
        "Layers can be recursively nested to create bigger computation blocks. Each layer will track the weights of its sublayers (both trainable and non-trainable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ-5mZN6HEi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's reuse the Linear class\n",
        "# with a `build` method that we defined above.\n",
        "\n",
        "class MLP(Layer):\n",
        "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.linear_1 = Linear(32)\n",
        "        self.linear_2 = Linear(32)\n",
        "        self.linear_3 = Linear(10)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.linear_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return self.linear_3(x)\n",
        "\n",
        "mlp = MLP()\n",
        "\n",
        "# The first call to the `mlp` object will create the weights.\n",
        "y = mlp(tf.ones(shape=(3, 64)))\n",
        "\n",
        "# Weights are recursively tracked.\n",
        "assert len(mlp.weights) == 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts51nf-CHM5c",
        "colab_type": "text"
      },
      "source": [
        "## Built-in layers\n",
        "\n",
        "Keras provides you with a [wide range of built-in layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/), so that you don't have to implement your own layers all the time.\n",
        "\n",
        "- Convolution layers\n",
        "- Transposed convolutions\n",
        "- Separateable convolutions\n",
        "- Average and max pooling\n",
        "- Global average and max pooling\n",
        "- LSTM, GRU (with built-in cuDNN acceleration)\n",
        "- BatchNormalization\n",
        "- Dropout\n",
        "- Attention\n",
        "- ConvLSTM2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHf4p6MtHVJZ",
        "colab_type": "text"
      },
      "source": [
        "## The training argument in call\n",
        "Some layers, in particular the BatchNormalization layer and the Dropout layer, have different behaviors during training and inference. For such layers, it is standard practice to expose a training (boolean) argument in the call method.\n",
        "\n",
        "By exposing this argument in call, you enable the built-in training and evaluation loops (e.g. fit) to correctly use the layer in training and inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1WokdDiHRa5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dropout(Layer):\n",
        "  \n",
        "  def __init__(self, rate):\n",
        "    super(Dropout, self).__init__()\n",
        "    self.rate = rate\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    if training:\n",
        "      return tf.nn.dropout(inputs, rate=self.rate)\n",
        "    return inputs\n",
        "\n",
        "class MLPWithDropout(Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "      super(MLPWithDropout, self).__init__()\n",
        "      self.linear_1 = Linear(32)\n",
        "      self.dropout = Dropout(0.5)\n",
        "      self.linear_3 = Linear(10)\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "      x = self.linear_1(inputs)\n",
        "      x = tf.nn.relu(x)\n",
        "      x = self.dropout(x, training=training)\n",
        "      return self.linear_3(x)\n",
        "    \n",
        "mlp = MLPWithDropout()\n",
        "y_train = mlp(tf.ones((2, 2)), training=True)\n",
        "y_test = mlp(tf.ones((2, 2)), training=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7NGlKhIHfD5",
        "colab_type": "text"
      },
      "source": [
        "## A more Functional way of defining models\n",
        "To build deep learning models, you don't have to use object-oriented programming all the time. Layers can also be composed functionally, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBzfM8C9Hhnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use an `Input` object to describe the shape and dtype of the inputs.\n",
        "# This is the deep learning equivalent of *declaring a type*.\n",
        "# The shape argument is per-sample; it does not include the batch size.\n",
        "# The functional API focused on defining per-sample transformations.\n",
        "# The model we create will automatically batch the per-sample transformations,\n",
        "# so that it can be called on batches of data.\n",
        "inputs = tf.keras.Input(shape=(16,))\n",
        "\n",
        "# We call layers on these \"type\" objects\n",
        "# and they return updated types (new shapes/dtypes).\n",
        "x = Linear(32)(inputs) # We are reusing the Linear layer we defined earlier.\n",
        "x = Dropout(0.5)(x) # We are reusing the Dropout layer we defined earlier.\n",
        "outputs = Linear(10)(x)\n",
        "\n",
        "# A functional `Model` can be defined by specifying inputs and outputs.\n",
        "# A model is itself a layer like any other.\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# A functional model already has weights, before being called on any data.\n",
        "# That's because we defined its input shape in advance (in `Input`).\n",
        "assert len(model.weights) == 4\n",
        "\n",
        "# Let's call our model on some data.\n",
        "y = model(tf.ones((2, 16)))\n",
        "assert y.shape == (2, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrFHIH6JHq3L",
        "colab_type": "text"
      },
      "source": [
        "The Functional API tends to be more concise than subclassing, and provides a few other advantages (generally the same advantages that functional, typed languages provide over untyped OO development). However, it can only be used to define DAGs of layers -- recursive networks should be defined as `Layer` subclasses instead.\n",
        "\n",
        "Key differences between models defined via subclassing and Functional models are explained in [this blog post](https://medium.com/tensorflow/what-are-symbolic-and-imperative-apis-in-tensorflow-2-0-dfccecb01021).\n",
        "\n",
        "Learn more about the Functional API [here](https://www.tensorflow.org/alpha/guide/keras/functional).\n",
        "\n",
        "In your research workflows, you may often find yourself mix-and-matching OO models and Functional models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsYPkshJHv8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "\n",
        "model = Sequential([Linear(32), Dropout(0.5), Linear(10)])\n",
        "\n",
        "y = model(tf.ones((2, 16)))\n",
        "assert y.shape == (2, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkx79jfiH09a",
        "colab_type": "text"
      },
      "source": [
        "## Loss classes\n",
        "Keras features a wide range of built-in loss classes, like BinaryCrossentropy, CategoricalCrossentropy, KLDivergence, etc. They work like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9CSgnLyNL6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bce = tf.keras.losses.BinaryCrossentropy()\n",
        "y_true = [0., 0., 1., 1.]  # Targets\n",
        "y_pred = [1., 1., 1., 0.]  # Predictions\n",
        "loss = bce(y_true, y_pred)\n",
        "print('Loss:', loss.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyejy6x1NRbE",
        "colab_type": "text"
      },
      "source": [
        "Metric classes\n",
        "Keras also features a wide range of built-in metric classes, such as BinaryAccuracy, AUC, FalsePositives, etc.\n",
        "\n",
        "Unlike losses, metrics are stateful. You update their state using the update_state method, and you query the scalar metric result using result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPsGO6M9Ncti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = tf.keras.metrics.Accuracy()\n",
        "m.update_state([0, 1, 1, 1], [0, 1, 0, 0])\n",
        "print('Intermediate result: ', m.result().numpy())\n",
        "\n",
        "m.update_state([1, 1, 1, 1], [0, 1, 1, 0])\n",
        "print('Final result: ', m.result().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq3N0Cm4Nkew",
        "colab_type": "text"
      },
      "source": [
        "You can easily roll out your own metrics by subclassing the Metric class:\n",
        "\n",
        "- Create the state variables in __init__\n",
        "- Update the variables given y_true and y_pred in update_state\n",
        "- Return the metric result in result\n",
        "- Clear the state in reset_states\n",
        "\n",
        "Here's a quick implementation of a BinaryTruePositives metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eSFR5R3NtNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinaryTruePositives(tf.keras.metrics.Metric):\n",
        "\n",
        "  def __init__(self, name='binary_true_positives', **kwargs):\n",
        "    super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n",
        "    self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_true = tf.cast(y_true, tf.bool)\n",
        "    y_pred = tf.cast(y_pred, tf.bool)\n",
        "\n",
        "    values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
        "    values = tf.cast(values, self.dtype)\n",
        "    if sample_weight is not None:\n",
        "      sample_weight = tf.cast(sample_weight, self.dtype)\n",
        "      sample_weight = tf.broadcast_weights(sample_weight, values)\n",
        "      values = tf.multiply(values, sample_weight)\n",
        "    self.true_positives.assign_add(tf.reduce_sum(values))\n",
        "\n",
        "  def result(self):\n",
        "    return self.true_positives\n",
        "\n",
        "  def reset_states(self):\n",
        "    self.true_positive.assign(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72mJ1dfZNx7b",
        "colab_type": "text"
      },
      "source": [
        "Optimizer classes & a quick end-to-end training loop\n",
        "You don't normally have to define by hand how to update your variables during gradient descent, like we did in our initial linear regression example. You would usually use one of the built-in Keras optimizer, like SGD, RMSprop, or Adam.\n",
        "\n",
        "Here's a simple MNSIT example that brings together loss classes, metric classes, and optimizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpjkrf9UNyuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare a dataset.\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train[:].reshape(60000, 784).astype('float32') / 255\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Instantiate a simple classification model\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Instantiate a logistic loss function that expects integer targets.\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Instantiate an accuracy metric.\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# Instantiate an optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Iterate over the batches of the dataset.\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "  \n",
        "  # Open a GradientTape.\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Forward pass.\n",
        "    logits = model(x)\n",
        "\n",
        "    # Loss value for this batch.\n",
        "    loss_value = loss(y, logits)\n",
        "     \n",
        "  # Get gradients of weights wrt the loss.\n",
        "  gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "  \n",
        "  # Update the weights of our linear layer.\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  # Update the running accuracy.\n",
        "  accuracy.update_state(y, logits)\n",
        "  \n",
        "  # Logging.\n",
        "  if step % 100 == 0:\n",
        "    print('Step:', step)\n",
        "    print('Loss from last step:', float(loss_value))\n",
        "    print('Total running accuracy so far:', float(accuracy.result()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy63XItbODmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = x_test[:].reshape(10000, 784).astype('float32') / 255\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(128)\n",
        "\n",
        "accuracy.reset_states()  # This clears the internal state of the metric\n",
        "\n",
        "for step, (x, y) in enumerate(test_dataset):\n",
        "  logits = model(x)\n",
        "  accuracy.update_state(y, logits)\n",
        "\n",
        "print('Final test accuracy:', float(accuracy.result()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlBmKB9rORWx",
        "colab_type": "text"
      },
      "source": [
        "Using built-in training loops\n",
        "It would be a bit silly if you had to write your own low-level training loops every time for simple use cases. Keras provides you with a built-in training loop on the Model class. If you want to use it, either subclass from Model or create a Functional or Sequential model.\n",
        "\n",
        "To demonstrate it, let's reuse the MNIST setup from above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCh-LUXOSt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare a dataset.\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Instantiate a simple classification model\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Instantiate a logistic loss function that expects integer targets.\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Instantiate an accuracy metric.\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# Instantiate an optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDca0T0_QE97",
        "colab_type": "text"
      },
      "source": [
        "First, call compile to configure the optimizer, loss, and metrics to monitor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkoX_4PBQHM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ioudx5-QJCL",
        "colab_type": "text"
      },
      "source": [
        "Then we call fit on our model to pass it the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJEU4aF1QLk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(dataset, epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es7FFq0SQN6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = x_test[:].reshape(10000, 784).astype('float32') / 255\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(128)\n",
        "\n",
        "loss, acc = model.evaluate(test_dataset)\n",
        "print('loss:', loss, 'acc:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fQGew0DQRVn",
        "colab_type": "text"
      },
      "source": [
        "Note that you can also monitor your loss and metrics on some validation data during fit.\n",
        "\n",
        "Also, you can call fit directly on Numpy arrays, so no need for the dataset conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZnfoWfrQUen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "\n",
        "num_val_samples = 10000\n",
        "x_val = x_train[-num_val_samples:]\n",
        "y_val = y_train[-num_val_samples:]\n",
        "x_train = x_train[:-num_val_samples]\n",
        "y_train = y_train[:-num_val_samples]\n",
        "\n",
        "# Instantiate a simple classification model\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Instantiate a logistic loss function that expects integer targets.\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Instantiate an accuracy metric.\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# Instantiate an optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss,\n",
        "              metrics=[accuracy])\n",
        "model.fit(x_train, y_train,\n",
        "          validation_data=(x_val, y_val),\n",
        "          epochs=3,\n",
        "          batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0hvb_uFQbcJ",
        "colab_type": "text"
      },
      "source": [
        "Remember that TensorFlow and Keras don't represent a single workflow. It's a spectrum of workflows, each with its own trade-off between usability and flexibility. For instance, you've noticed that it's much easier to use fit than to write a custom training loop, but fit doesn't give you the same level of granular control for research use cases.\n",
        "\n",
        "So use the right tool for the job!\n",
        "\n",
        "A core principle of Keras is \"progressive disclosure of complexity\": it's easy to get started, and you can gradually dive into workflows where you write more and more logic from scratch, providing you with complete control.\n",
        "\n",
        "This applies to both model definition, and model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExsfT4dNW4C7",
        "colab_type": "text"
      },
      "source": [
        "## Building a Neural Network for Regression Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "356YSpstXeCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijgq7vhfYhuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Activation,Dropout\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frDNv67xXmOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tOw1H-iXy55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "petrol_cons = pd.read_csv(r'petrol_consumption.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1tXIEswX5jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "petrol_cons.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FSJzwmmYAIg",
        "colab_type": "text"
      },
      "source": [
        "You can see that there are five columns in the dataset. The regression model will be trained on the first four columns, i.e. Petrol_tax, Average_income, Paved_Highways, & Population_Driver_License. The value for the last column i.e. Petrol_Consumption will be predicted (Y). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yAlQx75YEf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = petrol_cons.iloc[:, 0:4].values\n",
        "y = petrol_cons.iloc[:, 4].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74b5tV1zYLjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_layer = Input(shape=(X.shape[1],))\n",
        "dense_layer_1 = Dense(100, activation='relu')(input_layer)\n",
        "dense_layer_2 = Dense(50, activation='relu')(dense_layer_1)\n",
        "dense_layer_3 = Dense(25, activation='relu')(dense_layer_2)\n",
        "output = Dense(1)(dense_layer_3)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output)\n",
        "model.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP0qmkWkYYdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=2, epochs=100, verbose=1, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcdleoSSYZhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "pred_train = model.predict(X_train)\n",
        "print(np.sqrt(mean_squared_error(y_train,pred_train)))\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "print(np.sqrt(mean_squared_error(y_test,pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWKvnEvnYp3w",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network for Classification Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgs2Llb3Zn5s",
        "colab_type": "text"
      },
      "source": [
        "Car Evaluation Database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX, M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates cars according to the following concept structure:\n",
        "\n",
        "CAR car acceptability\n",
        ". PRICE overall price\n",
        "\n",
        ". . buying buying price\n",
        "\n",
        ". . maint price of the maintenance\n",
        "\n",
        ". TECH technical characteristics\n",
        "\n",
        ". . COMFORT comfort\n",
        "\n",
        ". . . doors number of doors\n",
        "\n",
        ". . . persons capacity in terms of persons to carry\n",
        "\n",
        ". . . lug_boot the size of luggage boot\n",
        "\n",
        ". . safety estimated safety of the car\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWmOjVOkZ82x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J65yh6VSZ1wH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = ['price', 'maint', 'doors', 'persons', 'lug_capacity', 'safety','output']\n",
        "cars = pd.read_csv(r'car_evaluation.csv', names=cols, header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFkJravVaHwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cars.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjD1fLdkaQp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_size = plt.rcParams[\"figure.figsize\"]\n",
        "plot_size [0] = 8\n",
        "plot_size [1] = 6\n",
        "plt.rcParams[\"figure.figsize\"] = plot_size\n",
        "\n",
        "\n",
        "cars.output.value_counts().plot(kind='pie', autopct='%0.05f%%', colors=['lightblue', 'lightgreen', 'orange', 'pink'], explode=(0.05, 0.05, 0.05,0.05))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0VvXM_labip",
        "colab_type": "text"
      },
      "source": [
        "All the columns in our dataset are categorical. Deep learning is based on statistical algorithms and statistical algorithms work with numbers. Therefore, we need to convert the categorical information into numeric columns. There are various approaches to do that but one of the most common approach is one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN5KZlhDahCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "price = pd.get_dummies(cars.price, prefix='price')\n",
        "maint = pd.get_dummies(cars.maint, prefix='maint')\n",
        "\n",
        "doors = pd.get_dummies(cars.doors, prefix='doors')\n",
        "persons = pd.get_dummies(cars.persons, prefix='persons')\n",
        "\n",
        "lug_capacity = pd.get_dummies(cars.lug_capacity, prefix='lug_capacity')\n",
        "safety = pd.get_dummies(cars.safety, prefix='safety')\n",
        "\n",
        "labels = pd.get_dummies(cars.output, prefix='condition')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj2hoGkDamel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = pd.concat([price, maint, doors, persons, lug_capacity, safety] , axis=1)\n",
        "\n",
        "labels.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXp1G-rMapOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = labels.values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aYGWxBPa1m7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_layer = Input(shape=(X.shape[1],))\n",
        "dense_layer_1 = Dense(15, activation='relu')(input_layer)\n",
        "dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)\n",
        "output = Dense(y.shape[1], activation='softmax')(dense_layer_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74YlBO34a4iU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTeBjrI4a4d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=8, epochs=50, verbose=1, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x2S7WpabELn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUk254sCaM2G",
        "colab_type": "text"
      },
      "source": [
        "One can add more layers to the model with more nodes and see if you can get better results on the validation and test sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJP4wLY2X45_",
        "colab_type": "text"
      },
      "source": [
        "## Saving the Model in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzWABgsnX845",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a simple model\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Dense(10, activation='relu', input_shape=(32,)),\n",
        "  layers.Dense(10)\n",
        "])\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(data, labels, batch_size=32, epochs=5)\n",
        "\n",
        "\n",
        "# Save entire model to a HDF5 file\n",
        "model.save('my_model')\n",
        "\n",
        "# Recreate the exact same model, including weights and optimizer.\n",
        "model = tf.keras.models.load_model('my_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Iwdo1Y_Qtup",
        "colab_type": "text"
      },
      "source": [
        "## Building a Convolutional Neural Network with TensorFlow 2.0 + Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-umJSZoQwlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# Load Data & Remove color channels\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train)).shuffle(10000).batch(32)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NroyC9oqQ7b4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(Model):\n",
        "    def __init__(self,\n",
        "                 loss_object,\n",
        "                 optimizer,\n",
        "                 train_loss,\n",
        "                 train_metric,\n",
        "                 test_loss,\n",
        "                 test_metric):\n",
        "        '''\n",
        "            Setting all the variables for our model.\n",
        "        '''\n",
        "        super(MyModel, self).__init__()\n",
        "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
        "        self.flatten = Flatten()\n",
        "        self.d1 = Dense(128, activation='relu')\n",
        "        self.d2 = Dense(10, activation='softmax')\n",
        "\n",
        "        self.loss_object = loss_object\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loss = train_loss\n",
        "        self.train_metric = train_metric\n",
        "        self.test_loss = test_loss\n",
        "        self.test_metric = test_metric\n",
        "\n",
        "    def nn_model(self, x):\n",
        "        '''\n",
        "            Defining the architecture of our model. This is where we run \n",
        "            through our whole dataset and return it, when training and testing.\n",
        "        '''\n",
        "        x = self.conv1(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.d1(x)\n",
        "        return self.d2(x)\n",
        "    \n",
        "    @tf.function\n",
        "    def train_step(self, images, labels):\n",
        "        '''\n",
        "            This is a TensorFlow function, run once for each epoch for the\n",
        "            whole input. We move forward first, then calculate gradients with\n",
        "            Gradient Tape to move backwards.\n",
        "        '''\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.nn_model(images)\n",
        "            loss = self.loss_object(labels, predictions)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        self.train_loss(loss)\n",
        "        self.train_metric(labels, predictions)\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(self, images, labels):\n",
        "        '''\n",
        "            This is a TensorFlow function, run once for each epoch for the\n",
        "            whole input.\n",
        "        '''\n",
        "        predictions = self.nn_model(images)\n",
        "        t_loss = self.loss_object(labels, predictions)\n",
        "\n",
        "        self.test_loss(t_loss)\n",
        "        self.test_metric(labels, predictions)\n",
        "    \n",
        "    def fit(self, train, test, epochs):\n",
        "        '''\n",
        "            This fit function runs training and testing.\n",
        "        '''\n",
        "        for epoch in range(epochs):\n",
        "            for images, labels in train:\n",
        "                self.train_step(images, labels)\n",
        "\n",
        "            for test_images, test_labels in test:\n",
        "                self.test_step(test_images, test_labels)\n",
        "\n",
        "            template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "            print(template.format(epoch+1,\n",
        "                                  self.train_loss.result(),\n",
        "                                  self.train_metric.result()*100,\n",
        "                                  self.test_loss.result(),\n",
        "                                  self.test_metric.result()*100))\n",
        "\n",
        "            # Reset the metrics for the next epoch\n",
        "            self.train_loss.reset_states()\n",
        "            self.train_metric.reset_states()\n",
        "            self.test_loss.reset_states()\n",
        "            self.test_metric.reset_states()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UXRJWVGRJNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a loss object\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# Select the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Specify metrics for training\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "# Specify metrics for testing\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "# Create an instance of the model\n",
        "model = MyModel(loss_object = loss_object,\n",
        "                optimizer = optimizer,\n",
        "                train_loss = train_loss,\n",
        "                train_metric = train_metric,\n",
        "                test_loss = test_loss,\n",
        "                test_metric = test_metric)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "model.fit(train = train_ds,\n",
        "          test = test_ds,\n",
        "          epochs = EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwKQLIvhZO71",
        "colab_type": "text"
      },
      "source": [
        "Multiple GPUs\n",
        "tf.keras models can run on multiple GPUs using tf.distribute.Strategy. This API provides distributed training on multiple GPUs with almost no changes to existing code.\n",
        "\n",
        "Currently, tf.distribute.MirroredStrategy is the only supported distribution strategy. MirroredStrategy does in-graph replication with synchronous training using all-reduce on a single machine. To use distribute.Strategys , nest the optimizer instantiation and model construction and compilation in a Strategy's .scope(), then train the model.\n",
        "\n",
        "The following example distributes a tf.keras.Model across multiple GPUs on a single machine.\n",
        "\n",
        "First, define a model inside the distributed strategy scope:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P9rmjyLZT-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(layers.Dense(16, activation='relu', input_shape=(10,)))\n",
        "  model.add(layers.Dense(1))\n",
        "\n",
        "  optimizer = tf.keras.optimizers.SGD(0.2)\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yljJ6oKZZeB",
        "colab_type": "text"
      },
      "source": [
        "Now train as usual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm7VodhBZbMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.random.random((1024, 10))\n",
        "y = np.random.randint(2, size=(1024, 1))\n",
        "x = tf.cast(x, tf.float32)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
        "\n",
        "model.fit(dataset, epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}